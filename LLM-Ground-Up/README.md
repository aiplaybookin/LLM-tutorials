# Developing LLM wit Lit-GPT

Overview

image 1


## 2 DATA PREP
- Build Vocabulary : Text to tokens with token ids
- Tokens to Embeddings 



- TOKENIZER

BytePair Encoding (BPE)
    - Advantage : Can jandle unkown words
Ticktoken
    - Opensource by openai
    - 3x faster faster than original paper, and 6x faster than huggingface implementations

## 3 LLM Architecture fundamentals
- Differences in 

Basic run of how next token is generated....

## 4 Pre-training Fundamentals





# References
- https://github.com/rasbt/LLM-workshop-2024
- https://www.youtube.com/watch?v=quh7z1q7-uc
- https://github.com/rasbt/LLMs-from-scratch
